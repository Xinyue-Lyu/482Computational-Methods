{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p5dL4YeVorzV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VjCGDx3aeK_i"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "feGo1EKE4sRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564b17f1-dec3-4a5a-bb4a-bae5b61c77c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106\n",
            "12\n",
            "40\n"
          ]
        }
      ],
      "source": [
        "# Use the following code to load and normalize the dataset for training and testing\n",
        "# It will downlad the dataset into data subfolder (change to your data folder name)\n",
        "train_dataset = torchvision.datasets.FashionMNIST('data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST('data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "\n",
        "# Use the following code to create a validation set of 10%\n",
        "train_indices, val_indices, _, _ = train_test_split(\n",
        "    range(len(train_dataset)),\n",
        "    train_dataset.targets,\n",
        "    stratify=train_dataset.targets,\n",
        "    test_size=0.1,\n",
        ")\n",
        "\n",
        "# Generate training and validation subsets based on indices\n",
        "train_split = Subset(train_dataset, train_indices)\n",
        "val_split = Subset(train_dataset, val_indices)\n",
        "\n",
        "\n",
        "# set batches sizes\n",
        "train_batch_size = 512 #Define train batch size\n",
        "test_batch_size  = 256 #Define test batch size (can be larger than train batch size)\n",
        "\n",
        "\n",
        "# Define dataloader objects that help to iterate over batches and samples for\n",
        "# training, validation and testing\n",
        "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
        "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
        "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "num_train_batches=len(train_batches)\n",
        "num_val_batches=len(val_batches)\n",
        "num_test_batches=len(test_batches)\n",
        "\n",
        "\n",
        "print(num_train_batches)\n",
        "print(num_val_batches)\n",
        "print(num_test_batches)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ACAIGFCN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims, dropout_prob):\n",
        "        super(ACAIGFCN, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()  # BatchNorm layers\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Hidden Layers with BatchNorm and Dropout\n",
        "        for h_dim in hidden_dims:\n",
        "            layer = nn.Linear(prev_dim, h_dim)\n",
        "            self.layers.append(layer)\n",
        "            self.batch_norms.append(nn.BatchNorm1d(h_dim))  # Apply BatchNorm\n",
        "            self.dropouts.append(nn.Dropout(p=dropout_prob))  # Apply Dropout\n",
        "            prev_dim = h_dim\n",
        "\n",
        "        # Output Layer (no BatchNorm or Dropout)\n",
        "        self.output_layer = nn.Linear(prev_dim, output_dim)\n",
        "\n",
        "        # Apply Kaiming Uniform Initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(layer.bias)  # Initialize bias to zero\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.output_layer.weight, nonlinearity='linear')\n",
        "        nn.init.zeros_(self.output_layer.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer, batch_norm, dropout in zip(self.layers, self.batch_norms, self.dropouts):\n",
        "            input = layer(input)  # Linear layer\n",
        "            input = batch_norm(input)  # Normalize activations\n",
        "            input = torch.nn.functional.relu(input)  # Apply ReLU\n",
        "            input = dropout(input)  # Apply Dropout\n",
        "\n",
        "        input = self.output_layer(input)  # Final output\n",
        "        return input\n",
        "\n",
        "\n",
        "def FCN_accuracy(batchSize, test_batch_size, epochs,\n",
        "                  hidden_dim, learning_rate, optimizer,dropout):\n",
        "\n",
        "  train_batches = DataLoader(train_split, batch_size=batchSize, shuffle=True)\n",
        "  val_batches = DataLoader(val_split, batch_size=batchSize, shuffle=True)\n",
        "  test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "  # Initialize neural network model #[50, 64, 80]\n",
        "  model = ACAIGFCN(input_dim = 784, output_dim = 10, hidden_dims = hidden_dim, dropout_prob=dropout)\n",
        "\n",
        "  train_loss_list = np.zeros((epochs,))\n",
        "  validation_accuracy_list = np.zeros((epochs,))\n",
        "\n",
        "  # Define loss function  and optimizer\n",
        "  loss_func = torch.nn.CrossEntropyLoss() # Cross Entropy loss\n",
        "  optm = optimizer(model.parameters(), lr=learning_rate) # torch.optim.SGD\n",
        "\n",
        "  # Iterate over epochs, batches with progress bar and train+ validate the ACAIGFCN\n",
        "  # Track the loss and validation accuracy\n",
        "  for epoch in tqdm.trange(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # ACAIGFCN Training\n",
        "    for train_features, train_labels in train_batches:\n",
        "        # Set model into training mode\n",
        "        model.train()\n",
        "\n",
        "        # Reshape images into a vector\n",
        "        train_features = train_features.reshape(-1, 28*28)\n",
        "\n",
        "        # Reset gradients, Calculate training loss on model\n",
        "        # Perfrom optimization, back propagation\n",
        "        optm.zero_grad()\n",
        "        outputs = model(train_features)\n",
        "        loss = loss_func(outputs, train_labels)\n",
        "        loss.backward()\n",
        "        optm.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Record loss for the epoch\n",
        "    train_loss_list[epoch] = total_loss / len(train_batches)\n",
        "\n",
        "\n",
        "    # ACAIGFCN Validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for val_features, val_labels in val_batches:\n",
        "        # Telling PyTorch we aren't passing inputs to network for training purpose\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            # Reshape validation images into a vector\n",
        "            val_features = val_features.reshape(-1, 28*28)\n",
        "\n",
        "            # Compute validation outputs (targets)\n",
        "            # and compute accuracy\n",
        "            validation_outputs = model(val_features)\n",
        "\n",
        "            correct += (torch.argmax(validation_outputs, dim=1) == val_labels).sum().item()#.type(torch.FloatTensor)\n",
        "            total += val_labels.size(0)\n",
        "    validation_accuracy_list[epoch] = (correct / total) * 100\n",
        "            #validation_accuracy_list[epoch] += correct.mean().item()\n",
        "\n",
        "    #validation_accuracy_list[epoch] = validation_accuracy_list[epoch]/len(val_batches)\n",
        "\n",
        "    # Record accuracy for the epoch; print training loss, validation accuracy val_acc * 100:.2f\n",
        "    print(f\"Epoch {epoch+1}: Training Loss = {train_loss_list[epoch]:.4f}, Validation Accuracy: {validation_accuracy_list[epoch] :.2f}%\") #)\n",
        "\n",
        "\n",
        "  #Calculate accuracy on test set\n",
        "  t_correct = 0\n",
        "  t_total = 0\n",
        "  # Telling PyTorch we aren't passing inputs to network for training purpose\n",
        "  with torch.no_grad():\n",
        "      for test_features, test_labels in test_batches:\n",
        "        model.eval()\n",
        "        # Reshape test images into a vector\n",
        "        test_features = test_features.reshape(-1, 28*28)\n",
        "\n",
        "        # Get model predictions\n",
        "        t_outputs = model(test_features)\n",
        "        predicted = torch.argmax(t_outputs, dim=1)\n",
        "\n",
        "        # Count correct predictions\n",
        "        t_correct += (predicted == test_labels).sum().item()\n",
        "        t_total += test_labels.size(0)\n",
        "\n",
        "  # Compute total test accuracy\n",
        "  test_accuracy = (t_correct / t_total) * 100\n",
        "\n",
        "  return train_loss_list, validation_accuracy_list, test_accuracy\n"
      ],
      "metadata": {
        "id": "aOBsdKGqzPj-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_accuracy_list, test_accuracy = FCN_accuracy(\n",
        "    batchSize=400,\n",
        "    test_batch_size=156,\n",
        "    epochs=10,\n",
        "    hidden_dim=[100, 150, 100],\n",
        "    learning_rate=0.005,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    dropout=0.1  # Adjust dropout rate\n",
        "  )\n",
        "print(f'100K_FCN test_accuracy{test_accuracy}')\n",
        "#t = 2m2s, 100K_FCN test_accuracy 88.14999999999999"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6quNEz5ztW0",
        "outputId": "86818004-b08f-49ef-8178-4be6dad3231e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:12<01:52, 12.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss = 0.5325, Validation Accuracy: 86.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:27<01:49, 13.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss = 0.3797, Validation Accuracy: 86.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:38<01:28, 12.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss = 0.3441, Validation Accuracy: 87.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:50<01:15, 12.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss = 0.3171, Validation Accuracy: 86.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:02<01:00, 12.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss = 0.3010, Validation Accuracy: 88.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:14<00:47, 11.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss = 0.2879, Validation Accuracy: 89.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:25<00:35, 11.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss = 0.2711, Validation Accuracy: 88.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:37<00:23, 11.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss = 0.2601, Validation Accuracy: 88.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:49<00:11, 11.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss = 0.2511, Validation Accuracy: 89.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:00<00:00, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss = 0.2447, Validation Accuracy: 89.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100K_FCN test_accuracy88.14999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: FCN 50K and 200K\n",
        "train_loss_list_50K, validation_accuracy_list_50K, test_accuracy_50K = FCN_accuracy(\n",
        "    batchSize=400,\n",
        "    test_batch_size=156,\n",
        "    epochs=10,\n",
        "    hidden_dim=[70, 34],\n",
        "    learning_rate=0.005,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    dropout=0.1  # Adjust dropout rate\n",
        "  )\n",
        "print(f'50K_FCN test_accuracy{test_accuracy_50K}')\n",
        "#t = 1m54s, 50K_FCN test_accuracy87.69"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PSkjOIQ2E3B",
        "outputId": "e7f51cb0-c290-446f-e601-59c99cc8497b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:10<01:38, 10.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss = 0.6297, Validation Accuracy: 86.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:21<01:27, 10.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss = 0.4088, Validation Accuracy: 87.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:33<01:17, 11.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss = 0.3668, Validation Accuracy: 87.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:45<01:08, 11.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss = 0.3454, Validation Accuracy: 88.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:57<00:58, 11.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss = 0.3218, Validation Accuracy: 88.32%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:08<00:46, 11.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss = 0.3081, Validation Accuracy: 88.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:19<00:34, 11.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss = 0.2996, Validation Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:30<00:22, 11.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss = 0.2910, Validation Accuracy: 88.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:41<00:11, 11.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss = 0.2814, Validation Accuracy: 88.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:52<00:00, 11.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss = 0.2707, Validation Accuracy: 88.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50K_FCN test_accuracy87.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list_200K, validation_accuracy_list_200K, test_accuracy_200K = FCN_accuracy(\n",
        "    batchSize=400,\n",
        "    test_batch_size=156,\n",
        "    epochs=10,\n",
        "    hidden_dim=[200, 150, 100, 64],\n",
        "    learning_rate=0.005,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    dropout=0.1  # Adjust dropout rate\n",
        "  )\n",
        "print(f'200K_FCN test_accuracy: {test_accuracy_200K}')\n",
        "#t = 2m1s, 200K_FCN test_accuracy: 88.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXALBsBM2ulA",
        "outputId": "4d0c5a4a-e3f8-48f4-a788-a20b71fcd69a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:12<01:48, 12.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Training Loss = 0.5502, Validation Accuracy: 85.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:23<01:35, 11.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Training Loss = 0.3845, Validation Accuracy: 86.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:36<01:24, 12.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Training Loss = 0.3453, Validation Accuracy: 87.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:47<01:11, 12.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Training Loss = 0.3240, Validation Accuracy: 88.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:59<00:59, 11.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Training Loss = 0.3011, Validation Accuracy: 87.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:11<00:47, 11.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Training Loss = 0.2805, Validation Accuracy: 88.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:23<00:35, 11.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Training Loss = 0.2727, Validation Accuracy: 89.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:35<00:23, 11.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Training Loss = 0.2606, Validation Accuracy: 88.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:47<00:11, 11.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Training Loss = 0.2474, Validation Accuracy: 89.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:58<00:00, 11.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Training Loss = 0.2395, Validation Accuracy: 89.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200K_FCN test_accuracy: 88.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_filters=32, hidden_dim=100, output_size=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(num_filters * 2 * 7 * 7, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Train and evaluate function\n",
        "def train_evaluate(model, train_loader, test_loader, epochs=10, lr=0.005, optimizer_type=optim.Adam):\n",
        "    optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Define dataset loaders\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
        "train_loader = DataLoader(torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform), batch_size=400, shuffle=True)\n",
        "test_loader = DataLoader(torchvision.datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform), batch_size=156, shuffle=False)\n"
      ],
      "metadata": {
        "id": "I_Q00fUJh2IQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: CNN 100K Model\n",
        "import itertools\n",
        "import torch.nn.functional as F\n",
        "\n",
        "CNN_100K_model = CNN(num_filters=10, hidden_dim=90)\n",
        "\n",
        "lr_values = [0.001, 0.005, 0.01]\n",
        "optimizer_types = [optim.Adam, optim.SGD]\n",
        "\n",
        "def hyperparameter_tuning(train_loader, test_loader, epochs=10):\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    param_combinations = list(itertools.product(lr_values, optimizer_types))\n",
        "\n",
        "    for lr, optimizer_type in param_combinations:\n",
        "        print(f\"Training with lr={lr}, optimizer={optimizer_type}\")\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        accuracy = train_evaluate(CNN_100K_model, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_type=optimizer_type)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "        # Update the best parameters if we get a higher accuracy\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'lr': lr, 'optimizer_type': optimizer_type}\n",
        "\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "    print(\"Best accuracy:\", best_accuracy)\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "hyperparameter_tuning(train_loader, test_loader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ86uLAWieu8",
        "outputId": "0df2b871-e179-4bac-b74e-0feb3e37c978"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with lr=0.001, optimizer=<class 'torch.optim.adam.Adam'>\n",
            "Accuracy: 0.8932\n",
            "Training with lr=0.001, optimizer=<class 'torch.optim.sgd.SGD'>\n",
            "Accuracy: 0.8966\n",
            "Training with lr=0.005, optimizer=<class 'torch.optim.adam.Adam'>\n",
            "Accuracy: 0.9093\n",
            "Training with lr=0.005, optimizer=<class 'torch.optim.sgd.SGD'>\n",
            "Accuracy: 0.9171\n",
            "Training with lr=0.01, optimizer=<class 'torch.optim.adam.Adam'>\n",
            "Accuracy: 0.9058\n",
            "Training with lr=0.01, optimizer=<class 'torch.optim.sgd.SGD'>\n",
            "Accuracy: 0.9155\n",
            "Best hyperparameters: {'lr': 0.005, 'optimizer_type': <class 'torch.optim.sgd.SGD'>}\n",
            "Best accuracy: 0.9171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: CNN Variants (50K, 20K, 10K)\n",
        "cnn_50k = CNN(num_filters=8, hidden_dim=64)\n",
        "cnn_20k = CNN(num_filters=6, hidden_dim=32)\n",
        "cnn_10k = CNN(num_filters=4, hidden_dim=10)"
      ],
      "metadata": {
        "id": "m62LAExtigJT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_50k_acc = train_evaluate(cnn_50k, train_loader, test_loader,optimizer_type=torch.optim.SGD)\n",
        "print(f\"CNN 50K Accuracy: {cnn_50k_acc:.4f}\")\n",
        "#time = 2m15s, CNN 50K Accuracy: 0.8988"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvJG3DFjqpKI",
        "outputId": "f6759360-f857-4805-c9a9-3652f2d5cb77"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 50K Accuracy: 0.8988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_20k_acc = train_evaluate(cnn_20k, train_loader, test_loader, optimizer_type=torch.optim.SGD)\n",
        "\n",
        "print(f\"CNN 20K Accuracy: {cnn_20k_acc:.4f}\")\n",
        "#t = 2m32s\n",
        "#CNN 20K Accuracy:0.7938"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWKAklGqe7r",
        "outputId": "aace11b7-72fb-43c7-8e3d-2c6cf2ec0c15"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 20K Accuracy: 0.7938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_10k_acc = train_evaluate(cnn_10k, train_loader, test_loader, optimizer_type=torch.optim.SGD)\n",
        "print(f\"CNN 10K Accuracy: {cnn_10k_acc:.4f}\")\n",
        "#t = 2m29s,0.7875"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4mnnhpTqiSk",
        "outputId": "b754843d-89d3-4376-dd7e-dff2db8cd8be"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 10K Accuracy: 0.7875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EM6GQLv6j5uH"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}